<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <meta name="description" content="Learn how to optimize PyTorch gradient accumulation for distributed training. Discover techniques to prevent slowdowns and improve performance in multi-GPU setups.">
    <meta name="author" content="Zach Mueller">
    <meta name="keywords" content="PyTorch, gradient accumulation, distributed training, GPU optimization, machine learning, deep learning">
    <meta name="dcterms.date" content="2023-03-03">
    <meta name="generator" content="quarto-1.7.31">
    <meta name="robots" content="index, follow">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@TheZachMueller">
    <meta name="twitter:title" content="PyTorch Gradient Accumulation Guide">
    <meta name="twitter:description" content="Learn how to optimize PyTorch gradient accumulation for distributed training. Discover techniques to prevent slowdowns and improve performance in multi-GPU setups.">
    <meta property="og:title" content="PyTorch Gradient Accumulation Guide">
    <meta property="og:description" content="Learn how to optimize PyTorch gradient accumulation for distributed training. Discover techniques to prevent slowdowns and improve performance in multi-GPU setups.">
    <meta property="og:type" content="article">
    <meta name="theme-color" content="#ffffff">
    <link rel="canonical" href="https://muellerzr.github.io/gradient_accumulation.html">
    <meta name="quarto:offset" content="../">

    <title>PyTorch Gradient Accumulation Guide: Optimizing Multi-GPU Training Performance</title>

    <!-- Preload critical assets -->
    <link rel="preload" href="../site_libs/bootstrap/bootstrap-icons.woff" as="font" type="font/woff" crossorigin>
    <link rel="preload" href="../site_libs/bootstrap/bootstrap-cfa207c77eb7cbba8b42f7d389f7d0a8.min.css" as="style">
    <link rel="preload" href="../site_libs/bootstrap/bootstrap.min.js" as="script">

    <!-- Critical CSS -->
    <link rel="stylesheet" href="../site_libs/bootstrap/bootstrap-cfa207c77eb7cbba8b42f7d389f7d0a8.min.css">
    <link rel="stylesheet" href="../site_libs/bootstrap/bootstrap-icons.css">
    <link rel="stylesheet" href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css">
    <link rel="stylesheet" href="../styles.css">

    <!-- Critical JavaScript -->
    <script src="../site_libs/quarto-nav/quarto-nav.js"></script>
    <script src="../site_libs/quarto-nav/headroom.min.js"></script>

    <!-- Deferred JavaScript -->
    <script defer src="../site_libs/clipboard/clipboard.min.js"></script>
    <script defer src="../site_libs/quarto-search/autocomplete.umd.js"></script>
    <script defer src="../site_libs/quarto-search/fuse.min.js"></script>
    <script defer src="../site_libs/quarto-search/quarto-search.js"></script>
    <script defer src="../site_libs/quarto-html/quarto.js" type="module"></script>
    <script defer src="../site_libs/quarto-html/popper.min.js"></script>
    <script defer src="../site_libs/quarto-html/tippy.umd.min.js"></script>
    <script defer src="../site_libs/quarto-html/anchor.min.js"></script>
    <script defer src="../site_libs/bootstrap/bootstrap.min.js"></script>

    <!-- Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-99XP3R051T"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-99XP3R051T', { 
            'anonymize_ip': true,
            'page_path': window.location.pathname
        });
    </script>

    <!-- Schema.org metadata -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "TechArticle",
        "headline": "PyTorch Gradient Accumulation Guide: Optimizing Multi-GPU Training Performance",
        "datePublished": "2023-03-03",
        "author": {
            "@type": "Person",
            "name": "Zach Mueller",
            "url": "https://muellerzr.github.io"
        },
        "description": "Learn how to optimize PyTorch gradient accumulation for distributed training. Discover techniques to prevent slowdowns and improve performance in multi-GPU setups.",
        "keywords": "PyTorch, gradient accumulation, distributed training, GPU optimization",
        "publisher": {
            "@type": "Organization",
            "name": "Zach Mueller's Blog",
            "url": "https://muellerzr.github.io"
        }
    }
    </script>

    <!-- Quarto search options -->
    <script id="quarto-search-options" type="application/json">{
      "location": "navbar",
      "copy-button": false,
      "collapse-after": 3,
      "panel-placement": "end",
      "type": "overlay",
      "limit": 50,
      "language": {
        "search-no-results-text": "No results",
        "search-matching-documents-text": "matching documents",
        "search-copy-link-title": "Copy link to search",
        "search-hide-matches-text": "Hide additional matches",
        "search-more-match-text": "more match in this document",
        "search-more-matches-text": "more matches in this document",
        "search-clear-button-title": "Clear",
        "search-detached-cancel-button-title": "Cancel",
        "search-submit-button-title": "Submit",
        "search-label": "Search"
      }
    }</script>
</head>

<body class="nav-sidebar floating nav-fixed fullcontent quarto-light">
    <!-- Skip to main content link for accessibility -->
    <a href="#quarto-document-content" class="skip-link" tabindex="0">Skip to main content</a>

    <!-- Search Results Container -->
    <div id="quarto-search-results" role="region" aria-label="Search results" aria-live="polite"></div>

    <!-- Primary Navigation -->
    <header id="quarto-header" class="headroom fixed-top" role="banner">
        <nav class="navbar navbar-expand-lg" data-bs-theme="dark" role="navigation" aria-label="Main navigation">
            <div class="navbar-container container-fluid">
                <!-- Brand -->
                <div class="navbar-brand-container mx-auto">
                    <a class="navbar-brand" href="../index.html">
                        <span class="navbar-title">Zach Mueller</span>
                    </a>
                </div>

                <!-- Search -->
                <div id="quarto-search" class="" title="Search" role="search"></div>

                <!-- Navigation Toggle -->
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" 
                        data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" 
                        aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Main Navigation Links -->
                <nav class="collapse navbar-collapse" id="navbarCollapse">
                    <ul class="navbar-nav navbar-nav-scroll me-auto" role="menubar">
                        <li class="nav-item" role="none">
                            <a class="nav-link" href="../index.html" role="menuitem">
                                <span class="menu-text">About Me</span>
                            </a>
                        </li>
                        <li class="nav-item" role="none">
                            <a class="nav-link" href="../blog/index.html" role="menuitem">
                                <span class="menu-text">Blog</span>
                            </a>
                        </li>
                        <li class="nav-item" role="none">
                            <a class="nav-link" href="../til/index.html" role="menuitem">
                                <span class="menu-text">Today I Learned</span>
                            </a>
                        </li>
                        <li class="nav-item" role="none">
                            <a class="nav-link" href="../builds/index.html" role="menuitem">
                                <span class="menu-text">PC Builds</span>
                            </a>
                        </li>
                    </ul>

                    <!-- Social Links -->
                    <ul class="navbar-nav navbar-nav-scroll ms-auto" role="menubar">
                        <li class="nav-item compact" role="none">
                            <a class="nav-link" href="https://github.com/muellerzr" role="menuitem">
                                <i class="bi bi-github" role="img" aria-label="GitHub"></i>
                            </a>
                        </li>
                        <li class="nav-item compact" role="none">
                            <a class="nav-link" href="https://twitter.com/TheZachMueller" role="menuitem">
                                <i class="bi bi-twitter" role="img" aria-label="Twitter"></i>
                            </a>
                        </li>
                    </ul>
                </nav>
            </div>
        </nav>

        <!-- Secondary Navigation -->
        <nav class="quarto-secondary-nav" role="navigation" aria-label="Secondary navigation">
            <div class="container-fluid d-flex">
                <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" 
                        role="button" data-bs-target=".quarto-sidebar-collapse-item" 
                        aria-controls="quarto-sidebar" aria-expanded="false" 
                        aria-label="Toggle sidebar navigation">
                    <i class="bi bi-layout-text-sidebar-reverse"></i>
                </button>
                <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb">
                    <ol class="breadcrumb">
                        <li class="breadcrumb-item">PyTorch Gradient Accumulation Guide: Optimizing Multi-GPU Training Performance</li>
                    </ol>
                </nav>
            </div>
        </nav>
    </header>

    <!-- Main Content Area -->
    <div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
        <!-- Sidebar -->
        <aside id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto" role="complementary">
        </aside>
        <div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>

        <!-- Main Content -->
        <main class="content" id="quarto-document-content" role="main">
            <article>
                <!-- Article Header -->
                <header id="title-block-header" class="quarto-title-block default">
                    <div class="quarto-title">
                        <h1 class="title">PyTorch Gradient Accumulation Guide: Optimizing Multi-GPU Training Performance</h1>
                        <div class="quarto-categories">
                            <div class="quarto-category">pytorch</div>
                        </div>
                    </div>

                    <div class="quarto-title-meta">
                        <div>
                            <div class="quarto-title-meta-heading">Published</div>
                            <div class="quarto-title-meta-contents">
                                <time datetime="2023-03-03">March 3, 2023</time>
                            </div>
                        </div>
                    </div>
                </header>

                <!-- Article Content -->
                <section id="introduction" class="level1">
                    <h1>Introduction</h1>
                    <p>Recently I was helping someone at work debug some distributed code as they were looking to find ways to speed it up. Immediately I noticed something odd, <em>gradient accumulation</em>.</p>
                    <p>That in of itself is not odd. But when it comes to <em>distributed compute</em> with <em>Pytorch</em>, if you are not careful you can see <strong>immense</strong> slowdowns in your code.</p>
                    <p>What follows below is an exploratory analysis I performed using Hugging Face <a href="https://github.com/huggingface/accelerate">Accelerate</a>, PyTorch Distributed, and three machines to test what and by how much is the <strong>optimal and correct setup for gradient accumulation on multiple GPUs</strong>.</p>
                    <section id="setup" class="level2">
                        <h2 class="anchored" data-anchor-id="setup">Setup</h2>
                        <p>First let's discuss setup.</p>
                        <p>For these experiments, I used the following:</p>
                        <ul>
                            <li>Python: 3.9.13</li>
                            <li>PyTorch: v1.12.1+cu113</li>
                            <li>Accelerate: v0.16.0</li>
                            <li>Transformers: v4.26.1</li>
                            <li>Compute:
                            <ul>
                                <li>Two single-GPU T4 nodes from GCP that can communicate to each other</li>
                                <li>One node with two T4 GPUs</li>
                            </ul></li>
                            <li>Script-specific parameters:
                            <ul>
                                <li>Batch size per GPU: 16</li>
                                <li>Gradient accumulation steps: 4</li>
                                <li>Total observed batch size (16<em>2</em>4): 128</li>
                                <li>Mixed precision: fp16</li>
                            </ul></li>
                            <li>Scripts: <a href="https://github.com/muellerzr/timing_experiments">available here</a></li>
                        </ul>
                    </section>
                    <section id="gradient-accumulation-is-special" class="level2">
                        <h2 class="anchored" data-anchor-id="gradient-accumulation-is-special">Gradient Accumulation is special?</h2>
                        <p>Let's talk about <em>why</em> gradient accumulation is different on multiple GPUs. On a single GPU, everything happens on that device, you can accumulate, compute, and update the gradients all exceedingly quickly. However when multiple GPUs get involved (both on a single network and on a single machine), each time the backward pass is performed <strong>all GPUs communicate with each other</strong>. The gradients are updated based on the <em>average</em> between each model on each GPU, and all the weights are synchronized to be this new result based on the average.</p>
                        <p>As you can imagine, for every instance you need to have all your GPUs communicate there will be a time loss. Even if they are in the same machine!</p>
                        <p>This time loss can be <strong>deadly</strong> to your programs as you run them because it can lead to even a <em>2x</em> slowdown!</p>
                        <p>So, what's the cure?</p>
                        <p>In PyTorch distributed training, the model is wrapped in a <code>DistributedDataParallel</code> class. This module is what stores the model and understands how to update and process these weight changes, and communicate between all the GPUs you are utilizing during training to do so. This update, as mentioned earlier, happens on <em>backward()</em>, but <strong>begins</strong> on the <em>forward pass</em>.</p>
                        <p>As a result, the <code>DistributedDataParallel</code> class has a function called <a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync"><code>no_sync</code></a>. Essentially this tells PyTorch while this block of code is running, <em>do not synchronize with the other GPUs</em>.</p>
                        <p>To make this work, this wrapper needs to be around both the forward and backward pass, such that:</p>
                        <div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> MyModel()</span>
                        <span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> DistributedDataParallel(net,...)</span>
                        <span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> net.no_sync():</span>
                        <span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> net(<span class="bu">input</span>)</span>
                        <span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_func(pred)</span>
                        <span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    pred.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
                        <p>To synchronize again, remove the <code>no_sync</code> wrapper for a batch and processes will synchronize again.</p>
                        <p>Translated, this is what gradient accumulation looks like properly in native PyTorch:</p>
                        <div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step, (x,y) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
                        <span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> gradient_accumulation_steps <span class="op">!=</span> <span class="dv">0</span>:</span>
                        <span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.no_sync():</span>
                        <span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(x)</span>
                        <span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_func(outputs, y)</span>
                        <span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss <span class="op">/</span> gradient_accumulation_steps</span>
                        <span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>            accelerator.backward(loss)</span>
                        <span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
                        <span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(x)</span>
                        <span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_func(outputs, y)</span>
                        <span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss <span class="op">/</span> gradient_accumulation_steps</span>
                        <span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        accelerator.backward(loss)</span>
                        <span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
                        <span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        lr_scheduler.step()</span>
                        <span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
                        <p>But just how important is this?</p>
                        <p>Can I just wrap around <code>.backward()</code> with the <code>no_sync</code>?</p>
                        <p>I ran a few experiments to figure <em>exactly that out</em>.</p>
                    </section>
                    <section id="the-experiments" class="level2">
                        <h2 class="anchored" data-anchor-id="the-experiments">The Experiments</h2>
                        <p>Each experiment ran through 29 total batches, using <code>bert-base-cased</code> as the model and the <code>mrpc</code> dataset. Each attempt was then ran 5 times and the average was taken.</p>
                        <p>I'll highlight each individual result below, as well as their code changes.</p>
                        <section id="the-baseline" class="level3">
                            <h3 class="anchored" data-anchor-id="the-baseline">The Baseline</h3>
                            <p>The <a href="https://github.com/muellerzr/timing_experiments/blob/main/baseline.py">baseline</a> consists of nothing special. It calls <code>.backward</code> at every step, and if we are finished accumulating then the optimizer and scheduler are zero'd and stepped.</p>
                            <div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step, (x,y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader):</span>
                            <span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(x)</span>
                            <span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_func(outputs, y)</span>
                            <span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss <span class="op">/</span> gradient_accumulation_steps</span>
                            <span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    accelerator.backward(loss)</span>
                            <span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> gradient_accumulation_steps <span class="op">==</span> <span class="dv">0</span>:</span>
                            <span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
                            <span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        lr_scheduler.step()</span>
                            <span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
                            <p>The <code>Accelerator</code> here is simply used to handle the standard DDP processes, and nothing more.</p>
                            <p>This baseline finished at:</p>
                            <blockquote class="blockquote">
                                <p>Note: Times are in Seconds per Batch</p>
                            </blockquote>
                            <table class="caption-top table">
                                <thead>
                                    <tr class="header">
                                        <th></th>
                                        <th>Multi Node</th>
                                        <th>Single Node</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr class="odd">
                                        <td>Run 1</td>
                                        <td>1.95</td>
                                        <td>0.52</td>
                                    </tr>
                                    <tr class="even">
                                        <td>Run 2</td>
                                        <td>2.11</td>
                                        <td>0.5</td>
                                    </tr>
                                    <tr class="odd">
                                        <td>Run 3</td>
                                        <td>1.94</td>
                                        <td>0.5</td>
                                    </tr>
                                    <tr class="even">
                                        <td><strong>Average</strong></td>
                                        <td><strong>2±0.01s</strong></td>
                                        <td><strong>0.50±0.01s</strong></td>
                                    </tr>
                                </tbody>
                            </table>
                            <p>Overall 2 seconds per batch on multi-node, and 0.5 seconds per batch on a single node. That's a <em>4x</em> slowdown when comparing single to multi-node. That is not efficient at all!</p>
                            <p>So, let's try using this fancy <code>no_sync</code> thing</p>
                        </section>
                        <section id="using-no_sync-improperly" class="level3">
                            <h3 class="anchored" data-anchor-id="using-no_sync-improperly">Using <code>no_sync</code>, improperly</h3>
                            <p>For <code>no_sync</code> to work correctly, it needs to be wrapped around <em>both the backward pass and forward pass</em>. Otherwise, processes will still be synchronized during <code>.backward()</code>.</p>
                            <p>Here is the bad example of what not to do, and its results:</p>
                            <div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step, batch <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader):</span>
                            <span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>        batch.to(accelerator.device)</span>
                            <span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>batch)</span>
                            <span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> outputs.loss</span>
                            <span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss <span class="op">/</span> gradient_accumulation_steps</span>
                            <span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> step <span class="op">%</span> gradient_accumulation_steps <span class="op">!=</span> <span class="dv">0</span>:</span>
                            <span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> model.no_sync():</span>
                            <span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>                accelerator.backward(loss)</span>
                            <span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
                            <span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>            accelerator.backward(loss)</span>
                            <span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
                            <span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            lr_scheduler.step()</span>
                            <span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
                            <blockquote class="blockquote">
                                <p>Note: Times are in Seconds per Batch</p>
                            </blockquote>
                            <table class="caption-top table">
                                <thead>
                                    <tr class="header">
                                        <th></th>
                                        <th>Multi Node</th>
                                        <th>Single Node</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr class="odd">
                                        <td>Run 1</td>
                                        <td>2.08</td>
                                        <td>0.52</td>
                                    </tr>
                                    <tr class="even">
                                        <td>Run 2</td>
                                        <td>2.09</td>
                                        <td>0.5</td>
                                    </tr>
                                    <tr class="odd">
                                        <td>Run 3</td>
                                        <td>2.23</td>
                                        <td>0.5</td>
                                    </tr>
                                    <tr class="even">
                                        <td>Average</td>
                                        <td>2.13±0.08s</td>
                                        <td>0.50±0.01s</td>
                                    </tr>
                                </tbody>
                            </table>
                            <p>As you can see, negligible different because it's not actually doing any non-synchronization! Everything is still being synced at the same time, and there's potential some amount of extra communication is being added on top of this considering on average it was .13s slower.</p>
                        </section>
                        <section id="what-is-the-right-way-then" class="level3">
                            <h3 class="anchored" data-anchor-id="what-is-the-right-way-then">What is the <em>right</em> way then?</h3>
                            <p>The <em>correct</em> way to use <code>no_sync</code>, as mentioned earlier, is to wrap around <strong>both</strong> the forward and backward pass. This ensures that only when we <em>break out</em> of the <code>no_sync</code> will the gradients fully be synchronized properly.</p>
                            <p>The snippet and results are below:</p>
                            <div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step, (x,y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader):</span>
                            <span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> gradient_accumulation_steps <span class="op">!=</span> <span class="dv">0</span>:</span>
                            <span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> model.no_sync():</span>
                            <span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(x)</span>
                            <span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_function(outputs, y)</span>
                            <span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss <span class="op">/</span> gradient_accumulation_steps</span>
                            <span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>            accelerator.backward(loss)</span>
                            <span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
                            <span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>batch)</span>
                            <span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, y)</span>
                            <span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss <span class="op">/</span> gradient_accumulation_steps</span>
                            <span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        accelerator.backward(loss)</span>
                            <span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
                            <span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        lr_scheduler.step()</span>
                            <span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
                            <blockquote class="blockquote">
                                <p>Note: Times are in Seconds per Batch</p>
                            </blockquote>
                            <table class="caption-top table">
                                <thead>
                                    <tr class="header">
                                        <th></th>
                                        <th>Multi Node</th>
                                        <th>Single Node</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr class="odd">
                                        <td>Run 1</td>
                                        <td>0.84</td>
                                        <td>0.4</td>
                                    </tr>
                                    <tr class="even">
                                        <td>Run 2</td>
                                        <td>1.04</td>
                                        <td>0.43</td>
                                    </tr>
                                    <tr class="odd">
                                        <td>Run 3</td>
                                        <td>0.86</td>
                                        <td>0.41</td>
                                    </tr>
                                    <tr class="even">
                                        <td>Average</td>
                                        <td>0.91±0.11s</td>
                                        <td>0.41±0.015s</td>
                                    </tr>
                                </tbody>
                            </table>
                            <p>You can see that <em>not only</em> did we get a <strong>2x speedup</strong> on the multi-node setup, but there was <em>also</em> a 25% speedup on the single node!</p>
                            <p>Reducing the amount of communication between all of your GPUs when training in a distributed process is <em>paramount</em> to training fast and efficiently.</p>
                            <p>The last script I will show is how Hugging Face Accelerate can do this automatically for you, using the <code>accumulate</code> wrapper:</p>
                        </section>
                        <section id="using-accelerate" class="level3">
                            <h3 class="anchored" data-anchor-id="using-accelerate">Using Accelerate!</h3>
                            <p>Snippet:</p>
                            <div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step, (x,y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader):</span>
                            <span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> accelerator.accumulate(model):</span>
                            <span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(x)</span>
                            <span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(outputs, y)</span>
                            <span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        accelerator.backward(loss)</span>
                            <span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
                            <span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        lr_scheduler.step()</span>
                            <span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
                            <p>Timings:</p>
                            <blockquote class="blockquote">
                                <p>Note: Times are in Seconds per Batch</p>
                            </blockquote>
                            <table class="caption-top table">
                                <thead>
                                    <tr class="header">
                                        <th></th>
                                        <th>Multi Node</th>
                                        <th>Single Node</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr class="odd">
                                        <td>Run 1</td>
                                        <td>0.84</td>
                                        <td>0.4</td>
                                    </tr>
                                    <tr class="even">
                                        <td>Run 2</td>
                                        <td>1.04</td>
                                        <td>0.43</td>
                                    </tr>
                                    <tr class="odd">
                                        <td>Run 3</td>
                                        <td>0.86</td>
                                        <td>0.41</td>
                                    </tr>
                                    <tr class="even">
                                        <td>Average</td>
                                        <td>0.91±0.11s</td>
                                        <td>0.41±0.015s</td>
                                    </tr>
                                </tbody>
                            </table>
                            <p>You can see that we get roughly the same times as the <code>no_sync</code> example showed earlier, however Accelerate let's us remove all of the if/else logic that was required entirely!</p>
                            <p>This helpful piece of magic not only lets you reduce lines of code, but it also ensures that <em>you can never see the slowdowns presented here</em>.</p>
                        </section>
                    </section>
                    <section id="article-takeaways" class="level2">
                        <h2 class="anchored" data-anchor-id="article-takeaways">Article Takeaways</h2>
                        <p>What I would like for you to take away from this brief discussion is:</p>
                        <ul>
                            <li>First, you should be <em>very</em> careful when writing distributed code, and try to minimize the number of times all your processes need to synchronize. This is one of the largest places a slowdown can occur, and it's not even limited by network!</li>
                            <li>Understand that even if something works the same on a single GPU, there may be behavioral changes and tweaks to have the same code working efficiently on other distributed systems. Accelerate helps with this by ensuring that the same code can be used across any distributed platform with minimal overhead on the user, however in general it is also a good idea to be familiar with just <em>what</em> needs to be changed and how</li>
                        </ul>
                        <p>If you liked this article, please be sure to check out my <a href="https://twitter.com/TheZachMueller">Twitter</a> and if you are interested be sure to check out Accelerate, a library I work on: <a href="https://github.com/huggingface/accelerate">Accelerate</a>.</p>
                    </section>
                </section>
            </article>
        </main>
    </div>

    <!-- Performance optimizations -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "TechArticle",
        "headline": "PyTorch Gradient Accumulation Guide: Optimizing Multi-GPU Training Performance",
        "datePublished": "2023-03-03",
        "author": {
            "@type": "Person",
            "name": "Zach Mueller",
            "url": "https://muellerzr.github.io"
        },
        "description": "Learn how to optimize PyTorch gradient accumulation for distributed training. Discover techniques to prevent slowdowns and improve performance in multi-GPU setups.",
        "keywords": "PyTorch, gradient accumulation, distributed training, GPU optimization",
        "publisher": {
            "@type": "Organization",
            "name": "Zach Mueller's Blog",
            "url": "https://muellerzr.github.io"
        }
    }
    </script>

    <!-- Intersection Observer for lazy loading -->
    <script>
        if ('IntersectionObserver' in window) {
            const images = document.querySelectorAll('img[loading="lazy"]');
            const imageObserver = new IntersectionObserver((entries, observer) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        const img = entry.target;
                        img.src = img.dataset.src;
                        img.removeAttribute('data-src');
                        observer.unobserve(img);
                    }
                });
            });
            images.forEach(img => imageObserver.observe(img));
        }
    </script>

    <!-- Quarto after-body scripts -->
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
        const icon = "";
        const anchorJS = new window.AnchorJS();
        anchorJS.options = {
            placement: 'right',
            icon: icon
        };
        anchorJS.add('.anchored');

        // Code annotation handling
        const isCodeAnnotation = (el) => {
            for (const clz of el.classList) {
                if (clz.startsWith('code-annotation-')) {                     
                    return true;
                }
            }
            return false;
        }

        // Copy button functionality
        const onCopySuccess = function(e) {
            const button = e.trigger;
            button.blur();
            button.classList.add('code-copy-button-checked');
            var currentTitle = button.getAttribute("title");
            button.setAttribute("title", "Copied!");
            setTimeout(function() {
                button.setAttribute("title", currentTitle);
                button.classList.remove('code-copy-button-checked');
            }, 1000);
            e.clearSelection();
        }

        // Initialize clipboard
        const clipboard = new window.ClipboardJS('.code-copy-button', {
            text: function(trigger) {
                const codeEl = trigger.previousElementSibling.cloneNode(true);
                for (const childEl of codeEl.children) {
                    if (isCodeAnnotation(childEl)) {
                        childEl.remove();
                    }
                }
                return codeEl.innerText;
            }
        });
        clipboard.on('success', onCopySuccess);

        // Handle external links
        const isInternal = (href) => {
            return href.startsWith(window.location.origin) || 
                   href.startsWith('/') || 
                   href.startsWith('#') || 
                   href.startsWith('mailto:');
        }

        document.querySelectorAll('a[href]').forEach(link => {
            if (!isInternal(link.href)) {
                link.setAttribute('target', '_blank');
                link.setAttribute('rel', 'noopener noreferrer');
            }
        });

        // Initialize tooltips
        if (typeof tippy !== 'undefined') {
            tippy('[data-tippy-content]');
        }
    });
    </script>
</body>
</html>